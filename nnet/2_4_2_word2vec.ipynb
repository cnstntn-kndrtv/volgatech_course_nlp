{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тема 2.3. Семантический нейросетевой анализ текста\n",
    "\n",
    "2.3.1. Векторное представление единиц текста.  \n",
    "**2.3.2. Обучение нейросетевой модели word2vec.**  \n",
    "2.3.3. Нейросетевая модель torch.embedding.  \n",
    "2.3.4. Анализ тематик электронных писем.  \n",
    "2.3.5. Анализ семантической близости двух текстов.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports in one cell\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation\n",
    "\n",
    "import graphviz\n",
    "from IPython import display\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some common settings\n",
    "plt.style.use('ggplot')\n",
    "graphviz.set_jupyter_format('svg')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"A word is characterized by the company it keeps\"  \n",
    "[John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth)  \n",
    "\n",
    "**Дистрибутивная гипотеза** - лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения (значение слова определяется словами, с которыми оно употребляется).  \n",
    "Например слова \"чай\" и \"кофе\" семантически близки так как употребляются вместе со словами \"ложка\", \"чашка\".  \n",
    "  \n",
    "**Языковая модель (статистическая)** - распределение вероятностей по последовательностям слов. Нейросетевые модели, обученные предсказывать слово с учетом контекста. Например: \"Мама мыла Х\", где слово \"раму\" имеет вероятность больше, чем слово \"папу\".  \n",
    "  \n",
    "**Word2Vec (word to vector)** - нейросетевая языковая модель обучения без учителя, получает на вход слово, на выходе - вектор фиксированной размерности. Задача модели - представить слова таким вектором, чтобы слова со схожим смыслом (контекстом) были близки, а с различающимся - далеки друг от друга.  \n",
    "Word2Vec обучается на больших объемах текстов (текстах новостей, википедии), что позволяет получить обширные \"знания\" (языковую интуицию). В результате модель можно переиспользовать на разных задачах.  \n",
    "  \n",
    "**Embedding ([вложение](https://ru.wikipedia.org/wiki/Вложение))** - векторное представление слова/текста (результат на выходе из модели).  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение собственной модели  word2vec\n",
    "\n",
    "Обучим собственную модель на небольшом корпусе текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корпус текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "он король\n",
      "она королева\n",
      "он мужчина\n",
      "она женщина\n",
      "москва это столица россии\n",
      "париж это столица франции\n",
      "берлин это столица германии\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "def delete_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Удаление стоп-слов.\n",
    "\n",
    "    Args:\n",
    "        text (str): Исходный текст.\n",
    "    Returns:\n",
    "        str: Обработанный текст.\n",
    "    \"\"\"\n",
    "\n",
    "    return ' '.join([t for t in tokenizer.tokenize(text) if t not in punctuation])\n",
    "\n",
    "\n",
    "corpora = [\n",
    "    'он - король',\n",
    "    'она - королева',\n",
    "    'он - мужчина',\n",
    "    'она - женщина',\n",
    "    'Москва это столица России',\n",
    "    'Париж это столица Франции',\n",
    "    'Берлин это столица Германии',\n",
    "]\n",
    "\n",
    "text_processors = [\n",
    "    str.lower,\n",
    "    delete_punctuation,\n",
    "]\n",
    "\n",
    "for proc in text_processors:\n",
    "    for i, text in enumerate(corpora):\n",
    "        corpora[i] = proc(text)\n",
    "\n",
    "print(*corpora, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание словаря.\n",
    "\n",
    "Первый шаг обучения модели - создание своего словаря.  \n",
    "Расширение словаря в модели Word2vec не предусматривается, в \"классическом\" варианте модель не может работать с несловарнымии словами.  \n",
    "Для того чтобы обойти это ограничение, можно обучить модель на побуквенных н-граммах (как это сделали [FastText](https://fasttext.cc) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['он', 'король'], ['она', 'королева'], ['он', 'мужчина'], ['она', 'женщина'], ['москва', 'это', 'столица', 'россии'], ['париж', 'это', 'столица', 'франции'], ['берлин', 'это', 'столица', 'германии']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpora = [tokenizer.tokenize(text) for text in corpora]\n",
    "print(tokenized_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx: {'париж': 0, 'мужчина': 1, 'столица': 2, 'франции': 3, 'россии': 4, 'это': 5, 'москва': 6, 'берлин': 7, 'она': 8, 'король': 9, 'он': 10, 'женщина': 11, 'королева': 12, 'германии': 13}\n",
      "idx2word: {0: 'париж', 1: 'мужчина', 2: 'столица', 3: 'франции', 4: 'россии', 5: 'это', 6: 'москва', 7: 'берлин', 8: 'она', 9: 'король', 10: 'он', 11: 'женщина', 12: 'королева', 13: 'германии'}\n",
      "vocabulary_size: 14\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set([tokens for sentence in tokenized_corpora for tokens in sentence])\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "print('word2idx:', word2idx)\n",
    "print('idx2word:', idx2word)\n",
    "print('vocabulary_size:', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дистрибутивная гипотеза** - лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения (значение слова определяется словами, с которыми оно употребляется).  \n",
    "Например слова \"чай\" и \"кофе\" семантически близки так как употребляются вместе со словами \"ложка\", \"чашка\".  \n",
    "\n",
    "Нам нужно создать наборы центральных слов (чай и кофе в примере выше) и слов контекста (ложка, чашка).  \n",
    "Для обучения модели используется искусственно созданный корпус.  \n",
    "Размер контекстного окна (`N`)- максимальное количество слов до и после центрального. Например для `N=2` для строки \"москва столица россии\", мы получим следующий набор:  \n",
    "Для центрального слова `москва` - `['столица', 'россии']`  \n",
    "Для центрального слова `столица` - `['москва', 'россии']`  \n",
    "  \n",
    "На практике (когда модель обучается на объемных корпусах) такая обработка производится на лету, мы же сделаем этот шаг отдельно.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_size = 2 # context window size - размер контекстного окна\n",
    "idx_pairs = []\n",
    "\n",
    "for sentence in tokenized_corpora:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # рассматриваем каждое слово как центральное\n",
    "    for center_word_i in range(len(indices)):\n",
    "        # для каждой позиции окна\n",
    "        for w in range(-cw_size, cw_size + 1):\n",
    "            context_word_i = center_word_i + w\n",
    "            if (context_word_i >= 0\n",
    "                and context_word_i < len(indices)\n",
    "                and center_word_i != context_word_i):\n",
    "                context_word_idx = indices[context_word_i]\n",
    "                idx_pairs.append((indices[center_word_i], context_word_idx))\n",
    "\n",
    "# для удобства переведем в numpy.array\n",
    "idx_pairs = np.array(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для текста: \"москва это столица россии\", получим:\n",
      "\n",
      "center\tcontext\n",
      "москва\tэто\n",
      "москва\tстолица\n",
      "это\tмосква\n",
      "это\tстолица\n",
      "это\tроссии\n",
      "столица\tмосква\n",
      "столица\tэто\n",
      "столица\tроссии\n"
     ]
    }
   ],
   "source": [
    "print(f'для текста: \"{corpora[4]}\", получим:', end='\\n\\n')\n",
    "print('center', 'context', sep='\\t')\n",
    "for p in idx_pairs[8:16]:\n",
    "    print(*[idx2word[i] for i in p], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Используются 2 архитектуры нейронных сетей - CBOW (Continuous Bag Of Word) и Skip-gram.  \n",
    "  \n",
    "## CBOW (Continuous Bag Of Word)  \n",
    "Получить самое вероятное центральное слово при условии заданного контекста.  \n",
    "$P(center|context, \\theta)$  \n",
    "Получить вероятностное распределение для пары \"центральное слово = слово контекста\" с некоторым параметром $\\theta$ (например $P(москва | столица)$)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.load_ipython_extensions([\n  \"nb-mermaid/nb-mermaid\"\n]);\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.load_ipython_extensions([\n",
    "  \"nb-mermaid/nb-mermaid\"\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD here[this page] --> bookmarklet(use bookmarklet) bookmarklet --> make here --> pip(pip) here --> clone pip --> installed clone(clone) --> installed clone --> fork(fork) fork --> profit((profit)) make(make diagrams) --> profit installed --> make\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схематично модель CBOW можно представить так:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'display_svg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2b/c72stsh96_3dj7x6m07sybtc0000gn/T/ipykernel_17240/1396275036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Схематично модель CBOW можно представить так:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_svg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'display_svg'"
     ]
    }
   ],
   "source": [
    "dot = \"\"\"\n",
    "digraph G{\n",
    "    rankdir=LR\n",
    "    node [shape=box]\n",
    "\n",
    "    w1 [label=\"w_i-2\"]\n",
    "    w2 [label=\"w_i-1\"]\n",
    "    w4 [label=\"w_i+1\"]\n",
    "    w5 [label=\"w_i+2\"]\n",
    "    w3 [label=\"w_i\"]\n",
    "    e [label=\"\"]\n",
    "    e -> w3\n",
    "    w1 -> e\n",
    "    w2 -> e\n",
    "    w4 -> e\n",
    "    w5 -> e\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print('Схематично модель CBOW можно представить так:')\n",
    "display.display_svg(graphviz.Source(dot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram  \n",
    "Для заданного центрального слова получить список слов контекста (инверсия CBOW).  \n",
    "$P(context|center, \\theta)$  \n",
    "  \n",
    "Мы будем реализовывать нашу модель на skip-gram. Давайте разберем подробнее.  \n",
    "Задача - получить максимум для всех пар контекст/центр.  \n",
    "$max \\prod _{center} \\prod _{context} P(context|center; \\theta)$  \n",
    "  \n",
    "Сумма вероятностей = 1, значит вероятность для несуществующих пар контекст/центр будет близка к 0. Нейронные сети минимизирую функцию потерь, а значит нам нужен $\\min$, а не $\\max$. Применим некоторые трансформации:  \n",
    "$\\min _{\\theta} - \\log \\prod _{center} \\prod _{context} P(context | center, \\theta)$\n",
    "  \n",
    "Заменим произведение логарифмов их суммой ($\\log(a * b) = \\log(a) + \\log(b)$) и поделим на количество пар ($T$).  \n",
    "$loss = - \\frac {1} {T} \\sum _{center} \\sum _{context} \\log P(context|center, \\theta)$  \n",
    "  \n",
    "  \n",
    "$P(context|center) = \\frac {\\exp (u ^{T} _{context} v _{center})} {\\sum _{w \\in vocab} \\exp(u ^{T} _{w} v _{center})}$  \n",
    "  \n",
    "где:  \n",
    "$\\frac {\\exp(.)} {\\sum \\exp(.)}$ - softmax  \n",
    "  \n",
    "$u ^{T} _{context} v _{center}$ - скалярное произведение векторов центрального ($v$) и контекстного ($u$) слов.  \n",
    "  \n",
    "$\\sum _{w \\in vocab}$ - итерация по всему словарю  \n",
    "  \n",
    "$u ^{T} _{w} v _{center}$ - вычисление сходства для заданного центрального слова и каждого слова словаря (использованного как слово контекста).  \n",
    "  \n",
    "Для каждой пары центр/контекст мы вычисляем оценку близости и делим на сумму всех возможных оценок близости заданного центрального слова и всех слов словаря (для определения относительной величины оценки близости в числителе). Softmax гарантирует что значения будут находиться в диапазоне от 0 до 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram - инверсия CBOW:\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"242pt\" height=\"206pt\"\n viewBox=\"0.00 0.00 242.00 206.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 202)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-202 238,-202 238,4 -4,4\"/>\n<!-- w1 -->\n<g id=\"node1\" class=\"node\">\n<title>w1</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"234,-198 180,-198 180,-162 234,-162 234,-198\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_i&#45;2</text>\n</g>\n<!-- w2 -->\n<g id=\"node2\" class=\"node\">\n<title>w2</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"234,-144 180,-144 180,-108 234,-108 234,-144\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_i&#45;1</text>\n</g>\n<!-- w4 -->\n<g id=\"node3\" class=\"node\">\n<title>w4</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"234,-90 180,-90 180,-54 234,-54 234,-90\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_i+1</text>\n</g>\n<!-- w5 -->\n<g id=\"node4\" class=\"node\">\n<title>w5</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"234,-36 180,-36 180,0 234,0 234,-36\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_i+2</text>\n</g>\n<!-- w3 -->\n<g id=\"node5\" class=\"node\">\n<title>w3</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"54,-117 0,-117 0,-81 54,-81 54,-117\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_i</text>\n</g>\n<!-- e -->\n<g id=\"node6\" class=\"node\">\n<title>e</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"144,-117 90,-117 90,-81 144,-81 144,-117\"/>\n</g>\n<!-- w3&#45;&gt;e -->\n<g id=\"edge1\" class=\"edge\">\n<title>w3&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.4,-99C62.39,-99 71.31,-99 79.82,-99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.92,-102.5 89.92,-99 79.92,-95.5 79.92,-102.5\"/>\n</g>\n<!-- e&#45;&gt;w1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>e&#45;&gt;w1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M137.97,-117.38C150.09,-128.54 165.73,-142.93 178.95,-155.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"176.69,-157.77 186.41,-161.97 181.43,-152.62 176.69,-157.77\"/>\n</g>\n<!-- e&#45;&gt;w2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>e&#45;&gt;w2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.4,-107.1C152.48,-109.58 161.51,-112.35 170.1,-114.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.33,-118.41 179.92,-118 171.39,-111.72 169.33,-118.41\"/>\n</g>\n<!-- e&#45;&gt;w4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>e&#45;&gt;w4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.4,-90.9C152.48,-88.42 161.51,-85.65 170.1,-83.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"171.39,-86.28 179.92,-80 169.33,-79.59 171.39,-86.28\"/>\n</g>\n<!-- e&#45;&gt;w5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>e&#45;&gt;w5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M137.97,-80.62C150.09,-69.46 165.73,-55.07 178.95,-42.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181.43,-45.38 186.41,-36.03 176.69,-40.23 181.43,-45.38\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot = \"\"\"\n",
    "digraph G{\n",
    "    rankdir=LR\n",
    "    node [shape=box]\n",
    "\n",
    "    w1 [label=\"w_i-2\"]\n",
    "    w2 [label=\"w_i-1\"]\n",
    "    w4 [label=\"w_i+1\"]\n",
    "    w5 [label=\"w_i+2\"]\n",
    "    w3 [label=\"w_i\"]\n",
    "    e [label=\"\"]\n",
    "    w3 -> e\n",
    "    e -> w1\n",
    "    e -> w2\n",
    "    e -> w4\n",
    "    e -> w5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print('Skip-gram - инверсия CBOW:')\n",
    "display.display_svg(graphviz.Source(dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_layer(word_idx: int,\n",
    "                    voc_size: int = vocabulary_size\n",
    "                    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-hot кодирование слова.\n",
    "\n",
    "    Выдает вектор длиной `voc_size`, значения которого равны 0,\n",
    "    за исключением `word_idx` = 1.\n",
    "\n",
    "    Args:\n",
    "        word_idx (int): индекс кодируемого слова\n",
    "        voc_size: (int, optional): размер словаря.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: бинарный вектор.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "get_input_layer(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.653172872568431\n",
      "Loss at epo 10: 4.241988020507913\n",
      "Loss at epo 20: 3.910935903850355\n",
      "Loss at epo 30: 3.6373354510257117\n",
      "Loss at epo 40: 3.407521368641602\n",
      "Loss at epo 50: 3.212881990169224\n",
      "Loss at epo 60: 3.047446501882453\n",
      "Loss at epo 70: 2.9065199494361877\n",
      "Loss at epo 80: 2.7860358749565326\n",
      "Loss at epo 90: 2.6823853434700715\n",
      "Loss at epo 100: 2.5924523370830634\n",
      "Loss at epo 110: 2.5136657898363315\n",
      "Loss at epo 120: 2.4439793150675926\n",
      "Loss at epo 130: 2.381794873821108\n",
      "Loss at epo 140: 2.3258725114558874\n",
      "Loss at epo 150: 2.2752445010762465\n",
      "Loss at epo 160: 2.2291447928077295\n",
      "Loss at epo 170: 2.1869590392238214\n",
      "Loss at epo 180: 2.1481875063557374\n",
      "Loss at epo 190: 2.112416204653288\n"
     ]
    }
   ],
   "source": [
    "# Скрытый слой \n",
    "# матрица весов W1 - для вычисления скрытого представления вектора v (центрального слова)\n",
    "# выдает вектор размером embedding_dims\n",
    "# матрица W1 имеет размер [embedding_dims, vocabulary_size]\n",
    "# в каждой колонке W1 хранится вектор для каждого слова из словаря\n",
    "\n",
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "\n",
    "# Выходной слой\n",
    "# должен иметь vocabulary_size нейронов так как он генерирует вероятности для каждого слова словаря.\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "\n",
    "        # софтмакс\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        # функция потерь\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epo % 10 == 0:\n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v: torch.Tensor, u: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"Вычисление меры близости.\"\"\"\n",
    "  return torch.dot(v,u) / (torch.norm(v)*torch.norm(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово           Сосед           Расстояние     \n",
      "---------------------------------------------\n",
      "париж           париж           1.0\n",
      "париж           берлин          0.81\n",
      "париж           женщина         0.65\n",
      "\n",
      "мужчина         мужчина         1.0\n",
      "мужчина         она             0.47\n",
      "мужчина         король          0.33\n",
      "\n",
      "столица         столица         1.0\n",
      "столица         франции         0.95\n",
      "столица         германии        0.88\n",
      "\n",
      "франции         франции         1.0\n",
      "франции         германии        0.97\n",
      "франции         столица         0.95\n",
      "\n",
      "россии          россии          1.0\n",
      "россии          германии        0.93\n",
      "россии          франции         0.86\n",
      "\n",
      "это             это             1.0\n",
      "это             она             0.63\n",
      "это             москва          0.4\n",
      "\n",
      "москва          москва          1.0\n",
      "москва          берлин          0.74\n",
      "москва          париж           0.59\n",
      "\n",
      "берлин          берлин          1.0\n",
      "берлин          женщина         0.86\n",
      "берлин          париж           0.81\n",
      "\n",
      "она             она             1.0\n",
      "она             это             0.63\n",
      "она             мужчина         0.47\n",
      "\n",
      "король          король          1.0\n",
      "король          мужчина         0.33\n",
      "король          это             0.24\n",
      "\n",
      "он              он              1.0\n",
      "он              столица         0.72\n",
      "он              франции         0.51\n",
      "\n",
      "женщина         женщина         1.0\n",
      "женщина         берлин          0.86\n",
      "женщина         париж           0.65\n",
      "\n",
      "королева        королева        1.0\n",
      "королева        германии        0.55\n",
      "королева        россии          0.54\n",
      "\n",
      "германии        германии        1.0\n",
      "германии        франции         0.97\n",
      "германии        россии          0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# {word: [(word, similarity)]}\n",
    "results: Dict[str, List[Tuple[str, float]]] = {}\n",
    "\n",
    "for w1 in vocabulary:\n",
    "    results[w1] = []\n",
    "    for w2 in vocabulary:\n",
    "        w1v = torch.matmul(W1, get_input_layer(word2idx[w1]))\n",
    "        w2v = torch.matmul(W1, get_input_layer(word2idx[w2]))\n",
    "        d = similarity(w1v, w2v).item()\n",
    "        results[w1].append((w2, d))\n",
    "\n",
    "    results[w1] = sorted(results[w1], key=lambda item: item[1], reverse=True)\n",
    "\n",
    "col_width = 15\n",
    "print(*[c.ljust(col_width) for c in ['Слово', 'Сосед', 'Расстояние']])\n",
    "print('-'*col_width*3)\n",
    "\n",
    "for w1 in results:\n",
    "    for i in range(3):\n",
    "        d = round(results[w1][i][1], 2)\n",
    "        print(w1.ljust(col_width), results[w1][i][0].ljust(col_width), d)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "1. Модель Word2Vec позволяет представить слово/текст вектором чисел.  \n",
    "2. Полученный вектор имеет фиксированную размерность и не зависит от размера словаря.  \n",
    "3. Модель можно обучать на больших корпусах текстов, это позволит переиспользовать одну модель для разных задач (будет рассмотрено в следующих темах).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительные материалы.  \n",
    "- [Mikolov, T., et al. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).](https://arxiv.org/abs/1301.3781)  \n",
    "- [Mikolov, Tomas, et al. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168 (2013).](http://arxiv.org/abs/1309.4168)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aeea92d5cbc7b4733b7a0b2ebc427ac21a58cceab802504a0dd243d8ac63dc4a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
